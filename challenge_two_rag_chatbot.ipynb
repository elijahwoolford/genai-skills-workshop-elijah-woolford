{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge Two: Aurora Bay FAQ RAG Chatbot\n",
        "\n",
        "This notebook implements a Retrieval-Augmented Generation (RAG) chatbot that answers questions about Aurora Bay using BigQuery ML and Vertex AI.\n",
        "\n",
        "## Key Technologies\n",
        "\n",
        "- **BigQuery** for storing FAQ data and embeddings\n",
        "- **BigQuery ML Remote Models** connected to Vertex AI `text-embedding-005`\n",
        "- **BigQuery VECTOR_SEARCH** for efficient similarity matching\n",
        "- **Gemini 2.5 Pro** for generating accurate, context-aware answers\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "1. **Setup**: Create BigQuery connection to Vertex AI and grant permissions\n",
        "2. **Data Import**: Load Aurora Bay FAQs from GCS (`gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv`) into BigQuery table\n",
        "3. **Embedding Model**: Create BigQuery ML remote model for `text-embedding-005`\n",
        "4. **Generate Embeddings**: Use `ML.GENERATE_EMBEDDING` to create embeddings for all FAQ pairs\n",
        "5. **Vector Search**: User asks question → `VECTOR_SEARCH` finds most similar FAQs by embedding distance\n",
        "6. **RAG Generation**: Pass retrieved FAQ context + user question to Gemini for final answer\n",
        "\n",
        "## Workflow\n",
        "\n",
        "```\n",
        "User Question\n",
        "    ↓\n",
        "ML.GENERATE_EMBEDDING (query embedding)\n",
        "    ↓\n",
        "VECTOR_SEARCH (find top-k similar FAQs)\n",
        "    ↓\n",
        "Retrieved FAQ Context\n",
        "    ↓\n",
        "Gemini 2.5 Pro (generate answer with context)\n",
        "    ↓\n",
        "Final Answer\n",
        "```\n"
      ],
      "metadata": {
        "id": "EARso2OmS8vH"
      },
      "id": "EARso2OmS8vH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "Install required packages for BigQuery, Vertex AI, and data processing."
      ],
      "metadata": {
        "id": "WS3uCverTCBL"
      },
      "id": "WS3uCverTCBL"
    },
    {
      "cell_type": "code",
      "id": "wKHZfwOvNLB5tK8IsV1s4P9E",
      "metadata": {
        "tags": [],
        "id": "wKHZfwOvNLB5tK8IsV1s4P9E"
      },
      "source": [
        "pip install --quiet --upgrade google-cloud-bigquery google-cloud-bigquery-connection google-cloud-aiplatform pandas db-dtypes\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import Libraries and Initialize Clients\n",
        "\n",
        "Import necessary libraries and initialize BigQuery and Vertex AI clients.\n"
      ],
      "metadata": {
        "id": "xqYCWp50Stip"
      },
      "id": "xqYCWp50Stip"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import aiplatform\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from typing import List, Dict\n",
        "\n",
        "# Configuration\n",
        "PROJECT_ID = \"qwiklabs-gcp-01-752385122246\"\n",
        "LOCATION = \"us-central1\"\n",
        "DATASET_ID = \"aurora_bay_dataset\"\n",
        "TABLE_ID = \"aurora_bay_faqs\"\n",
        "GCS_CSV_PATH = \"gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv\"\n",
        "\n",
        "# Initialize clients\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "print(f\"✓ Initialized BigQuery and Vertex AI for project: {PROJECT_ID}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcBpKT4xSxuL",
        "outputId": "d87869e4-9892-4861-b7bf-c3f1e15dc179"
      },
      "id": "mcBpKT4xSxuL",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Initialized BigQuery and Vertex AI for project: qwiklabs-gcp-01-752385122246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
            "  from google.cloud.aiplatform.utils import gcs_utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load CSV from GCS and Create BigQuery Table\n",
        "\n",
        "Load the Aurora Bay FAQs CSV file from Google Cloud Storage into a pandas DataFrame, then create a BigQuery dataset and table to store the data.\n"
      ],
      "metadata": {
        "id": "xpjuR1pFTJiA"
      },
      "id": "xpjuR1pFTJiA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV from GCS into pandas\n",
        "df = pd.read_csv(GCS_CSV_PATH)\n",
        "print(f\"✓ Loaded {len(df)} FAQ entries from GCS\")\n",
        "print(f\"\\nDataFrame columns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Create BigQuery dataset if it doesn't exist\n",
        "dataset_id = f\"{PROJECT_ID}.{DATASET_ID}\"\n",
        "try:\n",
        "    dataset = bigquery.Dataset(dataset_id)\n",
        "    dataset.location = LOCATION\n",
        "    dataset = bq_client.create_dataset(dataset, exists_ok=True)\n",
        "    print(f\"\\n✓ Dataset {DATASET_ID} ready\")\n",
        "except Exception as e:\n",
        "    print(f\"Dataset creation note: {e}\")\n",
        "\n",
        "# Define schema for the table\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"question\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"answer\", \"STRING\", mode=\"REQUIRED\"),\n",
        "]\n",
        "\n",
        "# Create table and load data\n",
        "table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    schema=schema,\n",
        "    write_disposition=\"WRITE_TRUNCATE\",  # Overwrite if exists\n",
        ")\n",
        "\n",
        "# Load DataFrame to BigQuery\n",
        "job = bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
        "job.result()  # Wait for the job to complete\n",
        "\n",
        "print(f\"✓ Loaded {job.output_rows} rows into {table_ref}\")\n",
        "\n",
        "# Verify data loaded correctly\n",
        "query = f\"SELECT COUNT(*) as count FROM `{table_ref}`\"\n",
        "result = bq_client.query(query).result()\n",
        "for row in result:\n",
        "    print(f\"✓ Verified: {row.count} rows in BigQuery table\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yypPvh2ATEhj",
        "outputId": "346ddd11-b11d-409e-bc6d-a7780392e161"
      },
      "id": "yypPvh2ATEhj",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded 50 FAQ entries from GCS\n",
            "\n",
            "DataFrame columns: ['question', 'answer']\n",
            "\n",
            "First few rows:\n",
            "                                         question  \\\n",
            "0                    When was Aurora Bay founded?   \n",
            "1           What is the population of Aurora Bay?   \n",
            "2      Where is the Aurora Bay Town Hall located?   \n",
            "3         Who is the current mayor of Aurora Bay?   \n",
            "4  What are the primary industries in Aurora Bay?   \n",
            "\n",
            "                                              answer  \n",
            "0  Aurora Bay was founded in 1901 by a group of f...  \n",
            "1  Aurora Bay has a population of approximately 3...  \n",
            "2  The Town Hall is located at 100 Harbor View Ro...  \n",
            "3  The current mayor is Linda Greenwood, elected ...  \n",
            "4  The primary industries include commercial fish...  \n",
            "\n",
            "✓ Dataset aurora_bay_dataset ready\n",
            "✓ Loaded 50 rows into qwiklabs-gcp-01-752385122246.aurora_bay_dataset.aurora_bay_faqs\n",
            "✓ Verified: 50 rows in BigQuery table\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Create BigQuery ML Embedding Model\n",
        "\n",
        "Create a remote model in BigQuery that connects to Vertex AI embeddings for generating vectors."
      ],
      "metadata": {
        "id": "1Hu8MQj7TRsC"
      },
      "id": "1Hu8MQj7TRsC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create BigQuery connection for remote models\n",
        "CONNECTION_ID = \"vertex-ai-connection\"\n",
        "\n",
        "from google.cloud import bigquery_connection_v1\n",
        "\n",
        "def create_bq_connection():\n",
        "    \"\"\"Create a BigQuery connection for Vertex AI remote models.\"\"\"\n",
        "    try:\n",
        "        connection_client = bigquery_connection_v1.ConnectionServiceClient()\n",
        "        parent = f\"projects/{PROJECT_ID}/locations/{LOCATION}\"\n",
        "\n",
        "        # Check if connection already exists\n",
        "        connection_name = f\"{parent}/connections/{CONNECTION_ID}\"\n",
        "        try:\n",
        "            existing = connection_client.get_connection(name=connection_name)\n",
        "            print(f\"✓ Connection already exists: {CONNECTION_ID}\")\n",
        "            return CONNECTION_ID\n",
        "        except:\n",
        "            pass  # Connection doesn't exist, create it\n",
        "\n",
        "        # Create new connection\n",
        "        connection = bigquery_connection_v1.Connection()\n",
        "        connection.cloud_resource = bigquery_connection_v1.CloudResourceProperties()\n",
        "\n",
        "        request = bigquery_connection_v1.CreateConnectionRequest(\n",
        "            parent=parent,\n",
        "            connection_id=CONNECTION_ID,\n",
        "            connection=connection,\n",
        "        )\n",
        "\n",
        "        created_connection = connection_client.create_connection(request=request)\n",
        "        print(f\"✓ Created BigQuery connection: {CONNECTION_ID}\")\n",
        "        return CONNECTION_ID\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Note: {e}\")\n",
        "        print(\"\\nAlternatively, create via command:\")\n",
        "        print(f\"!bq mk --connection --location={LOCATION} --project_id={PROJECT_ID} \\\\\")\n",
        "        print(f\"  --connection_type=CLOUD_RESOURCE {CONNECTION_ID}\")\n",
        "        return CONNECTION_ID\n",
        "\n",
        "# Create or verify connection exists\n",
        "connection_id = create_bq_connection()\n",
        "print(f\"\\nUsing connection: {LOCATION}.{connection_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pFvM-4SW0xO",
        "outputId": "0a0055cf-9f40-41a7-8d2a-68f13612429b"
      },
      "id": "_pFvM-4SW0xO",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Connection already exists: vertex-ai-connection\n",
            "\n",
            "Using connection: us-central1.vertex-ai-connection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grant Vertex AI User role to the connection service account\n",
        "from google.cloud import bigquery_connection_v1\n",
        "\n",
        "connection_client = bigquery_connection_v1.ConnectionServiceClient()\n",
        "connection_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/connections/{CONNECTION_ID}\"\n",
        "\n",
        "try:\n",
        "    connection = connection_client.get_connection(name=connection_name)\n",
        "    service_account = connection.cloud_resource.service_account_id\n",
        "\n",
        "    print(f\"Connection service account: {service_account}\")\n",
        "    print(\"\\nGranting Vertex AI User role...\")\n",
        "\n",
        "    # Grant the role using gcloud command\n",
        "    import subprocess\n",
        "\n",
        "    grant_cmd = [\n",
        "        \"gcloud\", \"projects\", \"add-iam-policy-binding\", PROJECT_ID,\n",
        "        f\"--member=serviceAccount:{service_account}\",\n",
        "        \"--role=roles/aiplatform.user\",\n",
        "        \"--condition=None\"\n",
        "    ]\n",
        "\n",
        "    result = subprocess.run(grant_cmd, capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(f\"✓ Granted roles/aiplatform.user to {service_account}\")\n",
        "    else:\n",
        "        print(f\"Note: {result.stderr}\")\n",
        "        print(\"\\nAlternatively, run this command:\")\n",
        "        print(f\"!gcloud projects add-iam-policy-binding {PROJECT_ID} \\\\\")\n",
        "        print(f\"  --member=serviceAccount:{service_account} \\\\\")\n",
        "        print(f\"  --role=roles/aiplatform.user\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nManually grant the Vertex AI User role to the service account shown in the error above.\")\n",
        "    print(\"Use: https://console.cloud.google.com/iam-admin/iam\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rbmep3M4Xhae",
        "outputId": "c6cd0bef-a45d-4e4d-f25a-83889b980a79"
      },
      "id": "Rbmep3M4Xhae",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connection service account: bqcx-11461060539-vixg@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
            "\n",
            "Granting Vertex AI User role...\n",
            "✓ Granted roles/aiplatform.user to bqcx-11461060539-vixg@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a BigQuery ML remote model that uses Vertex AI text embeddings\n",
        "embedding_model_id = f\"{PROJECT_ID}.{DATASET_ID}.embedding_model\"\n",
        "\n",
        "create_model_sql = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{embedding_model_id}`\n",
        "REMOTE WITH CONNECTION `{LOCATION}.{CONNECTION_ID}`\n",
        "OPTIONS (\n",
        "  ENDPOINT = 'text-embedding-005'\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "print(\"Creating BigQuery ML embedding model...\")\n",
        "bq_client.query(create_model_sql).result()\n",
        "print(f\"✓ Created embedding model: {embedding_model_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYux49_wTOCP",
        "outputId": "bfd547fe-266d-4f9f-e78e-5f5ae8bd2ad9"
      },
      "id": "VYux49_wTOCP",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating BigQuery ML embedding model...\n",
            "✓ Created embedding model: qwiklabs-gcp-01-752385122246.aurora_bay_dataset.embedding_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Generate Embeddings for FAQs\n",
        "\n",
        "Use BigQuery ML to generate embeddings for all FAQ entries and store them in a new table.\n"
      ],
      "metadata": {
        "id": "sji_bHOfTafp"
      },
      "id": "sji_bHOfTafp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for all FAQs using BigQuery ML\n",
        "# Combine question and answer for better semantic representation\n",
        "embedded_table_id = f\"{TABLE_ID}_embedded\"\n",
        "embedded_table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{embedded_table_id}\"\n",
        "\n",
        "generate_embeddings_sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{embedded_table_ref}` AS\n",
        "SELECT\n",
        "  question,\n",
        "  answer,\n",
        "  CONCAT(question, ' ', answer) AS content,\n",
        "  ml_generate_embedding_result\n",
        "FROM\n",
        "  ML.GENERATE_EMBEDDING(\n",
        "    MODEL `{embedding_model_id}`,\n",
        "    (SELECT question, answer, CONCAT(question, ' ', answer) AS content\n",
        "     FROM `{table_ref}`)\n",
        "  )\n",
        "\"\"\"\n",
        "\n",
        "print(\"Generating embeddings for all FAQs using BigQuery ML...\")\n",
        "job = bq_client.query(generate_embeddings_sql)\n",
        "result = job.result()\n",
        "\n",
        "print(f\"✓ Created table with embeddings: {embedded_table_ref}\")\n",
        "\n",
        "# Verify embeddings were generated\n",
        "verify_query = f\"\"\"\n",
        "SELECT question, ARRAY_LENGTH(ml_generate_embedding_result) as embedding_dim\n",
        "FROM `{embedded_table_ref}`\n",
        "LIMIT 5\n",
        "\"\"\"\n",
        "print(\"\\nVerifying embeddings:\")\n",
        "for row in bq_client.query(verify_query).result():\n",
        "    print(f\"  Question: {row.question[:60]}...\")\n",
        "    print(f\"  Embedding dimension: {row.embedding_dim}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm7F0VsATXpq",
        "outputId": "7addb42b-980e-4534-abde-f3a72ca85acc"
      },
      "id": "Xm7F0VsATXpq",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for all FAQs using BigQuery ML...\n",
            "✓ Created table with embeddings: qwiklabs-gcp-01-752385122246.aurora_bay_dataset.aurora_bay_faqs_embedded\n",
            "\n",
            "Verifying embeddings:\n",
            "  Question: What is the procedure for trash collection?...\n",
            "  Embedding dimension: 768\n",
            "\n",
            "  Question: What broadband or internet services are available?...\n",
            "  Embedding dimension: 768\n",
            "\n",
            "  Question: Are there any local newspapers or radio stations?...\n",
            "  Embedding dimension: 768\n",
            "\n",
            "  Question: What is the procedure for hosting an event at the waterfront...\n",
            "  Embedding dimension: 768\n",
            "\n",
            "  Question: How can I apply for a business license in Aurora Bay?...\n",
            "  Embedding dimension: 768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Implement Vector Search with VECTOR_SEARCH\n",
        "\n",
        "Use BigQuery's built-in VECTOR_SEARCH function to find similar FAQs based on embedding similarity.\n"
      ],
      "metadata": {
        "id": "YMsd9ndGUihX"
      },
      "id": "YMsd9ndGUihX"
    },
    {
      "cell_type": "code",
      "source": [
        "def search_similar_faqs(user_question: str, top_k: int = 5) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Search for similar FAQs using BigQuery VECTOR_SEARCH.\n",
        "\n",
        "    Args:\n",
        "        user_question: User's query text\n",
        "        top_k: Number of similar FAQs to return\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries with question, answer, and distance score\n",
        "    \"\"\"\n",
        "    # Use VECTOR_SEARCH with ML.GENERATE_EMBEDDING for the query\n",
        "    search_sql = f\"\"\"\n",
        "    SELECT\n",
        "        query.query,\n",
        "        base.question,\n",
        "        base.answer,\n",
        "        distance\n",
        "    FROM\n",
        "        VECTOR_SEARCH(\n",
        "            TABLE `{embedded_table_ref}`,\n",
        "            'ml_generate_embedding_result',\n",
        "            (\n",
        "                SELECT ml_generate_embedding_result, content AS query\n",
        "                FROM ML.GENERATE_EMBEDDING(\n",
        "                    MODEL `{embedding_model_id}`,\n",
        "                    (SELECT '{user_question}' AS content)\n",
        "                )\n",
        "            ),\n",
        "            top_k => {top_k},\n",
        "            options => '{{\"fraction_lists_to_search\": 0.01}}'\n",
        "        )\n",
        "    \"\"\"\n",
        "\n",
        "    results = bq_client.query(search_sql).result()\n",
        "\n",
        "    # Convert to list of dicts\n",
        "    similar_faqs = []\n",
        "    for row in results:\n",
        "        similar_faqs.append({\n",
        "            \"question\": row.question,\n",
        "            \"answer\": row.answer,\n",
        "            \"distance\": row.distance,\n",
        "            \"similarity\": 1 - row.distance  # Convert distance to similarity\n",
        "        })\n",
        "\n",
        "    return similar_faqs\n",
        "\n",
        "# Test the search function\n",
        "test_query = \"What are the operating hours?\"\n",
        "print(f\"Test search for: '{test_query}'\\n\")\n",
        "results = search_similar_faqs(test_query, top_k=5)\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"{i}. Distance: {result['distance']:.4f} | Similarity: {result['similarity']:.4f}\")\n",
        "    print(f\"   Q: {result['question']}\")\n",
        "    print(f\"   A: {result['answer'][:100]}...\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o9OSfxvTh0g",
        "outputId": "10c3059a-9594-4635-b5a2-36686179dfc3"
      },
      "id": "7o9OSfxvTh0g",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test search for: 'What are the operating hours?'\n",
            "\n",
            "1. Distance: 0.9096 | Similarity: 0.0904\n",
            "   Q: What are the operating hours of the Aurora Bay Public Library?\n",
            "   A: The library is open Monday through Friday from 9 AM to 6 PM, and on Saturdays from 10 AM to 4 PM. It...\n",
            "\n",
            "2. Distance: 0.9317 | Similarity: 0.0683\n",
            "   Q: When does the local fishermen’s market usually take place?\n",
            "   A: The fishermen’s market runs every Saturday from May through September at the Harborfront area, from ...\n",
            "\n",
            "3. Distance: 0.9328 | Similarity: 0.0672\n",
            "   Q: Are there specific quiet hours or noise ordinances?\n",
            "   A: Yes. Residential noise ordinances go into effect from 10 PM to 6 AM on weekdays and from 11 PM to 7 ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Build RAG Chatbot\n",
        "\n",
        "Implement the RAG pipeline with Gemini."
      ],
      "metadata": {
        "id": "S1FZ0E8PWKsk"
      },
      "id": "S1FZ0E8PWKsk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Gemini model for answering questions\n",
        "gemini_model = GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "def answer_question(user_question: str, top_k: int = 5, verbose: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Answer a user question using RAG with Aurora Bay FAQ data.\n",
        "\n",
        "    Args:\n",
        "        user_question: The user's question about Aurora Bay\n",
        "        top_k: Number of similar FAQs to retrieve for context\n",
        "        verbose: Print retrieved context if True\n",
        "\n",
        "    Returns:\n",
        "        Generated answer from Gemini based on retrieved FAQ context\n",
        "    \"\"\"\n",
        "    # Step 1: Search for similar FAQs\n",
        "    similar_faqs = search_similar_faqs(user_question, top_k=top_k)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Retrieved {len(similar_faqs)} similar FAQs:\\n\")\n",
        "        for i, faq in enumerate(similar_faqs, 1):\n",
        "            print(f\"{i}. [{faq['similarity']:.3f}] {faq['question']}\")\n",
        "        print()\n",
        "\n",
        "    # Step 2: Build context from retrieved FAQs\n",
        "    context_parts = []\n",
        "    for i, faq in enumerate(similar_faqs, 1):\n",
        "        context_parts.append(f\"FAQ {i}:\")\n",
        "        context_parts.append(f\"Q: {faq['question']}\")\n",
        "        context_parts.append(f\"A: {faq['answer']}\")\n",
        "        context_parts.append(\"\")  # Blank line\n",
        "\n",
        "    context = \"\\n\".join(context_parts)\n",
        "\n",
        "    # Step 3: Build prompt for Gemini\n",
        "    prompt = f\"\"\"You are a helpful assistant for Aurora Bay. Answer the user's question based ONLY on the provided FAQ context.\n",
        "If the FAQ context doesn't contain relevant information to answer the question, politely say you don't have that information.\n",
        "\n",
        "FAQ Context:\n",
        "{context}\n",
        "\n",
        "User Question: {user_question}\n",
        "\n",
        "Answer: Provide a clear, accurate answer based on the FAQ context above.\"\"\"\n",
        "\n",
        "    # Step 4: Generate answer with Gemini\n",
        "    response = gemini_model.generate_content(\n",
        "        prompt,\n",
        "        generation_config=GenerationConfig(\n",
        "            temperature=0.3,  # Lower temperature for more factual responses\n",
        "            top_p=0.95,\n",
        "            max_output_tokens=1024,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return response.text\n",
        "\n",
        "# Test the RAG chatbot\n",
        "test_question = \"What are the operating hours of Aurora Bay?\"\n",
        "print(f\"User Question: {test_question}\")\n",
        "print(\"=\"*60)\n",
        "answer = answer_question(test_question, verbose=True)\n",
        "print(\"=\"*60)\n",
        "print(f\"Answer:\\n{answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PygzIVEyWMZl",
        "outputId": "e6ca74cc-1fdc-484b-aab9-6dbc1a88c2f1"
      },
      "id": "PygzIVEyWMZl",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Question: What are the operating hours of Aurora Bay?\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 3 similar FAQs:\n",
            "\n",
            "1. [0.325] What are the operating hours of the Aurora Bay Public Library?\n",
            "2. [0.278] Does Aurora Bay have public transportation?\n",
            "3. [0.250] What is the average temperature range in Aurora Bay?\n",
            "\n",
            "============================================================\n",
            "Answer:\n",
            "Based on the information provided, here are the operating hours for specific services in Aurora Bay:\n",
            "\n",
            "*   **The Aurora Bay Public Library** is open Monday through Friday from 9 AM to 6 PM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Demo - Test the RAG Chatbot\n",
        "\n",
        "Test the chatbot with various Aurora Bay-related questions to demonstrate its capabilities.\n"
      ],
      "metadata": {
        "id": "cFi_jri0WUxI"
      },
      "id": "cFi_jri0WUxI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample questions to test the chatbot\n",
        "test_questions = [\n",
        "    \"What is the capital of France?\",  # Irrelevant - should return \"no information\"\n",
        "    \"What are the operating hours of the Aurora Bay Public Library?\",\n",
        "    \"What is the average temperature range in Aurora Bay?\",\n",
        "    \"Does Aurora Bay have public transportation?\",\n",
        "    \"What attractions and activities can I enjoy at Aurora Bay?\",\n",
        "]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"AURORA BAY FAQ CHATBOT SIM\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\n{'─'*70}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print('─'*70)\n",
        "\n",
        "    try:\n",
        "        answer = answer_question(question, top_k=5, verbose=False)\n",
        "        print(f\"\\nAnswer:\\n{answer}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Sim complete!\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cth7nP4AWWIP",
        "outputId": "6d08441f-c45d-4b4c-9738-a07c68c15ac9"
      },
      "id": "Cth7nP4AWWIP",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "AURORA BAY FAQ CHATBOT SIM\n",
            "======================================================================\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Question: What is the capital of France?\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Answer:\n",
            "I don't have that information.\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Question: What are the operating hours of the Aurora Bay Public Library?\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Answer:\n",
            "The Aurora Bay Public Library is open Monday through Friday from 9 AM to 6 PM, and on Saturdays from 10 AM to 4 PM. It is closed on Sundays and major holidays.\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Question: What is the average temperature range in Aurora Bay?\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Answer:\n",
            "Winters in Aurora Bay average between 10°F to 25°F, while summers are milder, around 50°F to 65°F.\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Question: Does Aurora Bay have public transportation?\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Answer:\n",
            "Yes. Aurora Bay operates a limited bus service on weekdays from 6 AM to 8 PM, servicing main routes including downtown, the airport, and residential neighborhoods.\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "Question: What attractions and activities can I enjoy at Aurora Bay?\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Answer:\n",
            "Based on the provided information, you can enjoy the following activities in Aurora Bay:\n",
            "\n",
            "*   Fishing\n",
            "*   Kayaking\n",
            "*   Hiking in the nearby forests\n",
            "*   Northern lights viewing in the winter\n",
            "*   Guided hikes, kayak tours, and wilderness camping experiences offered by local companies\n",
            "*   Sports activities and cultural events at the Aurora Bay Community Center\n",
            "\n",
            "======================================================================\n",
            "Sim complete!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ui2W_MhSYUgT"
      },
      "id": "Ui2W_MhSYUgT",
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "challenge_two"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}